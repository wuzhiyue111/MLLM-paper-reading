# **Paper Reading List**

A curated list of papers on Multi-Modal Large Language Models (MM-LLM), Unified Understanding & Generation, Self-Supervised Learning (SSL), and Diffusion Models (DiT).

*Feel free to recommend new papers by opening an issue!*

*A Library, Not a Feed: I believe we should aim to build a library, not a real-time feed. By focusing on seminal works that offer core insights, we can create a trusted, focused collection where every entry is a "must-read" for understanding true advancements. This would serve both newcomers and experts looking for what truly matters.*

---

### Audio

-   **USM**: Scaling Automatic Speech Recognition Beyond 100 Languages.
    [[arXiv]](https://arxiv.org/pdf/2303.01037)
-   **MusicFM(BEST-RQ)**: A Foundation Model for Music Informatics.
    [[arXiv]](https://arxiv.org/abs/2311.03318)
-   **Cosyvoice3**: Towards In-the-wild Speech Generation via Scaling-up and Post-training.
    [[arXiv]](https://arxiv.org/abs/2505.17589)
-   **NEST-RQ**: Next Token Prediction for Speech Self-Supervised Pre-Training
    [[arXiv]](https://arxiv.org/abs/2409.08680)
-   **MiniMax-Speech (Flow-VAE)**: Intrinsic Zero-Shot Text-to-Speech with a Learnable Speaker Encoder
    [[arXiv]](https://arxiv.org/pdf/2505.07916)
-   **DiTAR**: Diffusion Transformer Autoregressive Modeling for Speech Generation
    [[arXiv]](https://arxiv.org/pdf/2502.03930)
-   

### Visual

-   **show-o2**: Improved Native Unified Multimodal Models.
    [[arXiv]](https://arxiv.org/abs/2506.15564)
-   **MINT**: Multi-modal Chain of Thought in Unified Generative Models for Enhanced Image Generation.
    [[arXiv]](https://arxiv.org/pdf/2503.01298)
-   **BAGEL**: Emerging Properties in Unified Multimodal Pretraining. **(Recommended)**
    [[arXiv]](https://arxiv.org/abs/2505.14683)
-   **UniWorld-V1**: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation. **(Recommended)**
    [[arXiv]](https://arxiv.org/pdf/2506.03147)
-   **Nexus-Gen**: A Unified Model for Image Understanding, Generation, and Editing.
    [[arXiv]](https://arxiv.org/abs/2504.21356)
-   **WAN**:  OPEN AND ADVANCED LARGE-SCALE VIDEO GENERATIVE MODELS
    [[arXiv]](https://arxiv.org/pdf/2503.20314)
-   **MAGI-1**: Autoregressive Video Generation at Scale
    [[arXiv]](https://arxiv.org/pdf/2505.13211)
-   **Highly Compressed Tokenizer Can Generate Without Training**
    [[arXiv]](https://arxiv.org/abs/2506.08257)
-   **ASVR**: Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better
    [[arXiv]](https://arxiv.org/abs/2506.09040)
-   **BLIP3-o**: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset
    [[arXiv]](https://arxiv.org/abs/2505.09568)
    **Ovis-U1**: Ovis-U1 Technical Report
-   [[arXiv]]https://www.arxiv.org/pdf/2506.23044


### DiT

-   **Golden Noise**: Golden Noise for Diffusion Models: A Learning Framework.
    [[arXiv]](https://arxiv.org/pdf/2411.09502)
-   **REPA**: Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think
    [[arXiv]](https://arxiv.org/pdf/2410.06940)
-   **REPA-E**: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers.
    [[arXiv]](https://arxiv.org/abs/2504.10483)
-   **Diffuse and Disperse**: A method for improving diversity in generated images.
    [[arXiv]](https://arxiv.org/abs/2506.09027)
-   **Mean Flow**: Mean Flows for One-step Generative Modeling
    [[arXiv]](https://arxiv.org/pdf/2505.13447v1)
-   

### Explainable
-   **LANGUAGE MODELING IS COMPRESSION**
    [[arXiv]](https://arxiv.org/pdf/2309.10668)
-   
